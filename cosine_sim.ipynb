{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import isodate\n",
    "import json\n",
    "import pymysql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Extraction\n",
    "\n",
    "def get_video_ids(channel_id):\n",
    "  video_ids=[]\n",
    "  #creating a list named Videos_ids where it stores ids of all videos from the channel\n",
    "  video_ids=[]\n",
    "  # getting the playlist_id from channel details\n",
    "  response_playlist_id=youtube.channels().list(part=\"contentDetails\",id=channel_id).execute()\n",
    "  if 'items' in response_playlist_id and response_playlist_id['items']:\n",
    "    playlist_id=response_playlist_id['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    #initializing the value by None inorder to start with first token\n",
    "\n",
    "    request = youtube.playlistItems().list(part=\"snippet\",playlistId=playlist_id,maxResults=50)\n",
    "    response = request.execute()\n",
    "    for i in range(len(response['items'])):\n",
    "      video_ids.append(response['items'][i]['snippet']['resourceId']['videoId'])\n",
    "    return video_ids\n",
    "  else:\n",
    "    print(f\"Warning: No items found for channel ID: {channel_id}\")\n",
    "\n",
    "\n",
    "def get_data(channel_id,topic):\n",
    "  video_ids=get_video_ids(channel_id)\n",
    "  profile_response = youtube.channels().list(\n",
    "  part=\"snippet,statistics,brandingSettings,status,contentDetails\",   \n",
    "  id=channel_id).execute()\n",
    "  profile= profile_response['items'][0]['snippet']['thumbnails']['default']['url']\n",
    "  data=[]\n",
    "  no_tag=0\n",
    "  for ids in video_ids:\n",
    "    request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=ids)\n",
    "    response = request.execute()\n",
    "    try:\n",
    "      data.append({\n",
    "                  'topic':topic,\n",
    "                  'profile':profile,\n",
    "                  'channel_Name':response['items'][0]['snippet']['channelTitle'],\n",
    "                  'channel_id':response['items'][0]['snippet']['channelId'],\n",
    "                  'video_id':ids,\n",
    "                  'video_name':response['items'][0]['snippet']['title'],\n",
    "                  'title': response['items'][0]['snippet']['title'],\n",
    "                  'views':int(response['items'][0]['statistics']['viewCount']),\n",
    "                  'likes':int(response['items'][0]['statistics']['likeCount']),\n",
    "                  'video_description':response['items'][0]['snippet'].get('description', ''),\n",
    "                  'thumbnail':response['items'][0]['snippet']['thumbnails']['default']['url'],\n",
    "                  'tags':response['items'][0]['snippet'].get('tags', \"movie\")})\n",
    "    except:\n",
    "      no_tag = no_tag+1\n",
    "    if no_tag>0:\n",
    "      print(f\"Number of videos without tags: '{no_tag}' \")\n",
    "  return data\n",
    "\n",
    "#data source\n",
    "news_ids=['UC16niRr50-MSBwiO3YDb3RA','UCYPvAwZP8pZhSMW8qs7cVCw','UCeY0bbntWzzVIaj2z3QigXg','UC6RJ7-PaXg6TIH2BzZfTV7w','UCWCEYVwSqr7Epo6sSCfUgiw']\n",
    "sports_ids=['UC_WKb6N9iTGc77hxwXLDrbA','UCJ5v_MCY6GNUBTO8-D3XoAg','UCmqfX0S3x0I3uwLkPdpX03w','UCpcTrCXblq78GZrTUTLWeBw','UCqZQlzSHbVJrwrn5XvzrzcA']\n",
    "musics_ids=['UC_A7K2dXFsTMAciGmnNxy-Q','UCn4rEMqKtwBQ6-oEwbd4PcA','UCyyLMu6nnp0w2TaF5_kNBkg','UCNApqoVYJbYSrni4YsbXzyQ','UCLbdVvreihwZRL6kwuEUYsA']\n",
    "travel_ids=['UCDoHB7OsSqw-llDw5sZr7jw','UCvK4bOhULCpmLabd2pDMtnA','UCJUAqHnkDX15IvFoXIGTm2A','UCDQ0pYBbR3rGWnn-BZJ9lXA','UCrZx1iHMpaLoI3nDaLf0_QA']\n",
    "food_ids=['UC8Y-jrV8oR3s2Ix4viDkZtA','UCj4KP216972cPp2w_BAHy8g','UCHK357UDDmL6EMTb4YPE7ew','UCXw1ddyrUmib3zmCmvSI1ow','UCxAS_aK7sS2x_bqnlJHDSHw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Code\n",
    "api_key=\"AIzaSyB2TxX0C48PF7GfUu1KUOlBXKfN29fzHw0\"\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey=api_key)\n",
    "\n",
    "data=[]\n",
    "def fetch_details(list_ids,topic):\n",
    "  for id in list_ids:\n",
    "    list_dict=get_data(id,topic)\n",
    "    for dict_ in list_dict:\n",
    "      data.append(dict_)\n",
    "\n",
    "fetch_details(news_ids,'news')\n",
    "fetch_details(sports_ids,'sports')\n",
    "fetch_details(musics_ids,'musics')\n",
    "fetch_details(travel_ids,'travel')\n",
    "fetch_details(food_ids,'food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a JSON file\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Buckets: ['ashwin-fd669bfa-3182-43bf-9206-2b4fa300d559']\n",
      "Bucket 'ashwin-d2c2896b-ef40-49d1-968c-dd2188185371' not found. Creating bucket...\n",
      "Bucket 'ashwin-d2c2896b-ef40-49d1-968c-dd2188185371' created successfully.\n",
      "Error: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client\n",
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAVFIWJFEUESDJ6TY2',\n",
    "    aws_secret_access_key='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',\n",
    "    region_name='ap-south-1'  # Ensure this is the correct region for the bucket\n",
    ")\n",
    "bucket_name = 'ashwin-' + str(uuid.uuid4())  # or use a different unique name\n",
    "key = 'data.json'\n",
    "\n",
    "try:\n",
    "    # List all available buckets to confirm the bucket exists\n",
    "    buckets_response = s3.list_buckets()\n",
    "    bucket_names = [bucket['Name'] for bucket in buckets_response['Buckets']]\n",
    "    print(\"Available Buckets:\", bucket_names)\n",
    "\n",
    "    if bucket_name not in bucket_names:\n",
    "        print(f\"Bucket '{bucket_name}' not found. Creating bucket...\")\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': 'ap-south-1'}\n",
    "        )\n",
    "        print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "\n",
    "    # Load the JSON data from S3\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    print(\"Data loaded successfully from S3.\")\n",
    "    response = s3.upload_file('D:/DATA SCIENCE/Vs/data.json',Bucket=bucket_name,'data.json')\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Buckets: ['chusanth']\n",
      "Data loaded successfully from S3.\n"
     ]
    }
   ],
   "source": [
    "#pushing data to s3 \n",
    "import json\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAVFIWJFEUESDJ6TY2',\n",
    "    aws_secret_access_key='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',\n",
    "    region_name='ap-south-1'  # Ensure this is the correct region for the bucket\n",
    ")\n",
    "buckets_response = s3.list_buckets()\n",
    "bucket_names = [bucket['Name'] for bucket in buckets_response['Buckets']]\n",
    "# for bucket in buckets_response['Buckets']:\n",
    "#     print(bucket)\n",
    "print(\"Available Buckets:\", bucket_names)\n",
    "bucket_name='chusanth'\n",
    "if bucket_name not in bucket_names:\n",
    "    print(f\"Bucket '{bucket_name}' not found. Creating bucket...\")\n",
    "    s3.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration={'LocationConstraint': 'ap-south-1'}\n",
    "    )\n",
    "    print(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "\n",
    "response = s3.upload_file('D:/DATA SCIENCE/Vs/data.json',bucket_name,'data.json')\n",
    "print(\"Data loaded successfully from S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted successful\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data from S3 to local space\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAVFIWJFEUESDJ6TY2',\n",
    "    aws_secret_access_key='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',\n",
    "    region_name='ap-south-1'  # Ensure this is the correct region for the bucket\n",
    ")\n",
    "bucket_name = 'chusanth'\n",
    "raw_data_key = 'data.json'\n",
    "s3.download_file(bucket_name,raw_data_key,\"D:/DATA SCIENCE/Youtube_final/data.json\")\n",
    "print('Extracted successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json data from local space\n",
    "df_data=pd.read_json('D:/DATA SCIENCE/Youtube_final/data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#NLP-preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "wl=WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tags(tags):\n",
    "    if isinstance(tags, list):\n",
    "        tags = ' '.join(tags)\n",
    "    tokens = tags.split()\n",
    "    tokens = [wl.lemmatize(token.lower()) for token in tokens if token.lower() not in set(stopwords.words('english')).union(ENGLISH_STOP_WORDS)]\n",
    "    return ' '.join(tokens)\n",
    "    \n",
    "#preprocessing the tag column with lemmatization and removing stopwords\n",
    "df_data['tags']=df_data['tags'].apply(preprocess_tags)\n",
    "# Convert non-hashable types to strings to prevent issues with drop_duplicates()\n",
    "df_data = df_data.apply(lambda col: col.map(lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x))\n",
    "# Clean the data\n",
    "df_data.drop_duplicates(inplace=True)\n",
    "df_data.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "connection made\n",
      "preprossed data saved sucessfully.\n"
     ]
    }
   ],
   "source": [
    "#push the data to RDS\n",
    "import boto3\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "db_user = 'admin'\n",
    "db_password = 'chusanth0410'\n",
    "db_host = 'youtube-ml.cheo2e64wg9v.ap-south-1.rds.amazonaws.com' \n",
    "db_port = '3306'\n",
    "db_name = 'youtube'\n",
    "\n",
    "print('Connection started...')\n",
    "connection_string = f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "engine = create_engine(connection_string)\n",
    "print('Connection made...')\n",
    "df_data.to_sql('data_df', engine, if_exists='replace', index=False)\n",
    "print(\"Preprocessed data saved sucessfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection started...\n",
      "Connection made...\n",
      "Preprocessed data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "# Pull the data from RDS\n",
    "import boto3\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "db_user = 'admin'\n",
    "db_password = 'chusanth0410'\n",
    "db_host = 'youtube-ml.cheo2e64wg9v.ap-south-1.rds.amazonaws.com' \n",
    "db_port = '3306'\n",
    "db_name = 'youtube'\n",
    "\n",
    "print('Connection started...')\n",
    "connection_string = f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "engine = create_engine(connection_string)\n",
    "print('Connection made...')\n",
    "query = \"SELECT * FROM data_df\"\n",
    "# Pull the data from the database\n",
    "df_data = pd.read_sql(query, engine)\n",
    "print('Preprocessed data loaded sucessfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic', 'profile', 'channel_Name', 'channel_id', 'video_id',\n",
       "       'video_name', 'title', 'views', 'likes', 'video_description',\n",
       "       'thumbnail', 'tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[]\n",
    "for i in df_data['tags']:\n",
    "    tags.append(i)\n",
    "\n",
    "video_ids=[]\n",
    "for i in df_data['video_id']:\n",
    "    video_ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_input):\n",
    "    \n",
    "    # Initialize a TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the tags descriptions\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(tags)\n",
    "    # Transform the user input using the same vectorizer\n",
    "    query_vector = tfidf_vectorizer.transform([user_input])\n",
    "\n",
    "    # Calculate the cosine similarity between the input query and tags descriptions\n",
    "    cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Sort the videos by similarity (in descending order)\n",
    "    related_ids_indices = np.argsort(cosine_similarities)[::-1]\n",
    "\n",
    "    # Get the top 5 recommendations\n",
    "    top_n = 5\n",
    "    recommended_ids = [video_ids[i] for i in related_ids_indices][:top_n]\n",
    "    return recommended_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JVCptgMAzCY', 'SjuF-LeVjSE', 'eilQ19o_G60', 'Nw0Ie7RGp30', '2PZpIHPzdYo']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend('cake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check score\n",
    "tfidf_vectorizer = TfidfVectorizer() # Initialize a TF-IDF vectorizer\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tags)# Fit and transform the animal descriptions\n",
    "query_vector = tfidf_vectorizer.transform(['cake'])\n",
    "score = cosine_similarity(query_vector, tfidf_matrix).max()\n",
    "round(float(score)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m linear_kernel(\u001b[43mquery_vector\u001b[49m, tfidf_matrix)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m      2\u001b[0m df_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosin_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mcosine_similarities\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_vector' is not defined"
     ]
    }
   ],
   "source": [
    "cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "df_data['cosin_similarity']=cosine_similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
